{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "there are multiple strategies to handle missing values in our datasets. In our case (for categorical features) we have the following options:\n",
    "- removing samples that have missing feature values\n",
    "    - the problem with this approach is that it reduces the size of our dataset. The whole sample may get removed just because of one missing feature \n",
    "- impute the most frequent value\n",
    "    - we are replacing the missing values with the most frequent value of that feature in the dataset. so the values will be biased towards the filled values\n",
    "- treat the missing values as a separate feature value\n",
    "    - this may add some false patterns and spurious correlations because the original dataset didn't have this value\n",
    "\n",
    "For the general case, if the missing values are from numeric columns we have the following options:\n",
    "- removing samples that have missing feature values\n",
    "- replacing the missing values with the mean of the column\n",
    "- replacing the missing values with the median of the column\n",
    "- using regression to predict the missing values of each column based on the filled values\n",
    "- ...\n",
    "\n",
    "\n",
    "*we have to be carefull not to apply most frequent imputation on the whole dataset before splitting. we want the train, test and validation datasets to be isolated as much as possible so that the most frequent value of our test set won't affect the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"datasets/mushrooms.csv\")\n",
    "# train_data, val_test_data = train_test_split(data, train_size=0.7, random_state=42)\n",
    "# val_data, test_data = train_test_split(val_test_data, test_size=0.33, random_state=42)\n",
    "\n",
    "# imputer = SimpleImputer(strategy=\"most_frequent\", missing_values=\"?\")\n",
    "# train_data = pd.DataFrame(imputer.fit_transform(train_data), columns=train_data.columns)\n",
    "# val_data = pd.DataFrame(imputer.fit_transform(val_data), columns=val_data.columns)\n",
    "# test_data = pd.DataFrame(imputer.fit_transform(test_data), columns=test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/mushrooms.csv\")\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "    data.loc[:, data.columns != \"class\"], data[\"class\"], train_size=0.7, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val_test, y_val_test, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\", missing_values=\"?\")\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_val = pd.DataFrame(imputer.fit_transform(X_val), columns=X_val.columns)\n",
    "X_test = pd.DataFrame(imputer.fit_transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5686x116 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 125092 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated = pd.concat([X_train, X_val, X_test])\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(concatenated)\n",
    "\n",
    "\n",
    "# one_hot_encoded = pd.get_dummies(concatenated)\n",
    "\n",
    "# X_train_encoded = one_hot_encoded.iloc[:len(X_train)]\n",
    "# X_val_encoded = one_hot_encoded.iloc[len(X_train):len(X_train)+len(X_val)]\n",
    "# X_test_encoded = one_hot_encoded.iloc[len(X_train)+len(X_val):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desicion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc of validation with max depth of 4 = 0.9993876301285977\n",
      "acc of train with max depth of 4 = 0.9994723883221949\n",
      "acc of validation with max depth of 8 = 1.0\n",
      "acc of train with max depth of 8 = 1.0\n",
      "acc of validation with max depth of 16 = 1.0\n",
      "acc of train with max depth of 16 = 1.0\n",
      "acc of validation with max depth of 24 = 1.0\n",
      "acc of train with max depth of 24 = 1.0\n",
      "acc of validation with max depth of 32 = 1.0\n",
      "acc of train with max depth of 32 = 1.0\n",
      "acc of test with max depth of 8 = 1.0\n"
     ]
    }
   ],
   "source": [
    "max_depths = (4, 8, 16, 24, 32)\n",
    "best_acc = 0\n",
    "for md in max_depths:\n",
    "    dt_cls_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=md)\n",
    "    dt_cls_model = dt_cls_model.fit(enc.transform(X_train), y_train)\n",
    "    y_val_hat = dt_cls_model.predict(enc.transform(X_val))\n",
    "    y_train_hat = dt_cls_model.predict(enc.transform(X_train))\n",
    "\n",
    "    acc_val = sklearn.metrics.accuracy_score(y_val, y_val_hat)\n",
    "    acc_train = sklearn.metrics.accuracy_score(y_train, y_train_hat)\n",
    "\n",
    "    if acc_val > best_acc:\n",
    "        best_acc = acc_val\n",
    "        best_dt_cls_model = dt_cls_model\n",
    "        best_max_depth = md\n",
    "\n",
    "    print(f'acc of validation with max depth of {md} = {acc_val}')\n",
    "    print(f'acc of train with max depth of {md} = {acc_train}')\n",
    "\n",
    "# print(enc.transform(X_train).shape)\n",
    "# print(enc.transform(X_test).shape)\n",
    "y_test_hat = best_dt_cls_model.predict(enc.transform(X_test))\n",
    "acc_test = sklearn.metrics.accuracy_score(y_test, y_test_hat)\n",
    "print(f'acc of test with max depth of {best_max_depth} = {acc_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc of val with max depth of 4 and feature count 3 = 0.7281077770973668\n",
      "acc of train with max depth of 4 and feature count 3 = 0.7360182905381639\n",
      "acc of val with max depth of 4 and feature count 5 = 0.7924066135946112\n",
      "acc of train with max depth of 4 and feature count 5 = 0.8149841716496659\n",
      "acc of val with max depth of 4 and feature count 7 = 0.9632578077158603\n",
      "acc of train with max depth of 4 and feature count 7 = 0.9671122054168132\n",
      "acc of val with max depth of 8 and feature count 3 = 0.6472749540722597\n",
      "acc of train with max depth of 8 and feature count 3 = 0.645444952514949\n",
      "acc of val with max depth of 8 and feature count 5 = 0.9902020820575628\n",
      "acc of train with max depth of 8 and feature count 5 = 0.992085824832923\n",
      "acc of val with max depth of 8 and feature count 7 = 0.9571341090018372\n",
      "acc of train with max depth of 8 and feature count 7 = 0.9613084769609568\n",
      "acc of val with max depth of 12 and feature count 3 = 0.7262706674831598\n",
      "acc of train with max depth of 12 and feature count 3 = 0.730038691523039\n",
      "acc of val with max depth of 12 and feature count 5 = 0.898958971218616\n",
      "acc of train with max depth of 12 and feature count 5 = 0.8992261695392192\n",
      "acc of val with max depth of 12 and feature count 7 = 0.9626454378444581\n",
      "acc of train with max depth of 12 and feature count 7 = 0.9627154414351038\n",
      "acc of val with max depth of 16 and feature count 3 = 0.7336191059399878\n",
      "acc of train with max depth of 16 and feature count 3 = 0.7379528666901161\n",
      "acc of val with max depth of 16 and feature count 5 = 0.9828536436007348\n",
      "acc of train with max depth of 16 and feature count 5 = 0.9880408019697503\n",
      "acc of val with max depth of 16 and feature count 7 = 0.9565217391304348\n",
      "acc of train with max depth of 16 and feature count 7 = 0.9630671825536405\n",
      "acc of test with max depth of 8, feature count 5 = 0.9962732919254659\n"
     ]
    }
   ],
   "source": [
    "def select_random_features(X, feature_count):\n",
    "    columns = list(X.columns)\n",
    "    np.random.shuffle(columns)\n",
    "    return columns[:feature_count]\n",
    "\n",
    "\n",
    "max_depths = (4, 8, 12, 16)\n",
    "feature_counts = (3, 5, 7)\n",
    "best_acc = 0\n",
    "for md in max_depths:\n",
    "    for fc in feature_counts:\n",
    "        random_features = select_random_features(X_train, fc)\n",
    "        enc2 = OneHotEncoder()\n",
    "        enc2.fit(concatenated[random_features])\n",
    "\n",
    "        rf_cls_model = RandomForestClassifier(\n",
    "            n_estimators=7, criterion=\"gini\", max_depth=md\n",
    "        )\n",
    "        rf_cls_model = rf_cls_model.fit(\n",
    "            enc2.transform(X_train[random_features]), y_train\n",
    "        )\n",
    "        y_val_hat = rf_cls_model.predict(enc2.transform(X_val[random_features]))\n",
    "        y_train_hat = rf_cls_model.predict(enc2.transform(X_train[random_features]))\n",
    "\n",
    "        acc_val = sklearn.metrics.accuracy_score(y_val, y_val_hat)\n",
    "        acc_train = sklearn.metrics.accuracy_score(y_train, y_train_hat)\n",
    "\n",
    "        if acc_val > best_acc:\n",
    "            best_acc = acc_val\n",
    "            best_rf_cls_model = rf_cls_model\n",
    "            best_md, best_fc = md, fc\n",
    "            best_random_features = random_features\n",
    "            best_enc2 = enc2\n",
    "\n",
    "        print(f\"acc of val with max depth of {md} and feature count {fc} = {acc_val}\")\n",
    "        print(f\"acc of train with max depth of {md} and feature count {fc} = {acc_train}\")\n",
    "\n",
    "y_test_hat = best_rf_cls_model.predict(best_enc2.transform(X_test[best_random_features]))\n",
    "acc_test = sklearn.metrics.accuracy_score(y_test, y_test_hat)\n",
    "print(f'acc of test with max depth of {best_md}, feature count {best_fc} = {acc_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
