{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "there are multiple strategies to handle missing values in our datasets. In our case (for categorical features) we have the following options:\n",
    "- removing samples that have missing feature values\n",
    "    - the problem with this approach is that it reduces the size of our dataset. The whole sample may get removed just because of one missing feature \n",
    "- impute the most frequent value\n",
    "    - we are replacing the missing values with the most frequent value of that feature in the dataset. so the values will be biased towards the filled values\n",
    "- treat the missing values as a separate feature value\n",
    "    - this may add some false patterns and spurious correlations because the original dataset didn't have this value\n",
    "\n",
    "For the general case, if the missing values are from numeric columns we have the following options:\n",
    "- removing samples that have missing feature values\n",
    "- replacing the missing values with the mean of the column\n",
    "- replacing the missing values with the median of the column\n",
    "- using regression to predict the missing values of each column based on the filled values\n",
    "- ...\n",
    "\n",
    "\n",
    "*we have to be carefull not to apply most frequent imputation on the whole dataset before splitting. we want the train, test and validation datasets to be isolated as much as possible so that the most frequent value of our test set won't affect the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"datasets/mushrooms.csv\")\n",
    "# train_data, val_test_data = train_test_split(data, train_size=0.7, random_state=42)\n",
    "# val_data, test_data = train_test_split(val_test_data, test_size=0.33, random_state=42)\n",
    "\n",
    "# imputer = SimpleImputer(strategy=\"most_frequent\", missing_values=\"?\")\n",
    "# train_data = pd.DataFrame(imputer.fit_transform(train_data), columns=train_data.columns)\n",
    "# val_data = pd.DataFrame(imputer.fit_transform(val_data), columns=val_data.columns)\n",
    "# test_data = pd.DataFrame(imputer.fit_transform(test_data), columns=test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/mushrooms.csv\")\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "    data.loc[:, data.columns != \"class\"], data[\"class\"], train_size=0.7, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val_test, y_val_test, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\", missing_values=\"?\")\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_val = pd.DataFrame(imputer.fit_transform(X_val), columns=X_val.columns)\n",
    "X_test = pd.DataFrame(imputer.fit_transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_val.shape, y_val.shape)\n",
    "# print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desicion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc with max depth of 4 = 0.9993876301285977\n",
      "acc with max depth of 8 = 1.0\n",
      "acc with max depth of 16 = 1.0\n",
      "acc with max depth of 24 = 1.0\n",
      "acc with max depth of 32 = 1.0\n"
     ]
    }
   ],
   "source": [
    "max_depths = (4, 8, 16, 24, 32)\n",
    "for md in max_depths:\n",
    "    dt_cls_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=md)\n",
    "    dt_cls_model = dt_cls_model.fit(pd.get_dummies(X_train), y_train)\n",
    "    y_val_hat = dt_cls_model.predict(pd.get_dummies(X_val))\n",
    "\n",
    "    print(f'acc with max depth of {md} = {sklearn.metrics.accuracy_score(y_val, y_val_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc with max depth of 4 = 0.9185548071034905\n",
      "acc with max depth of 8 = 0.9987752602571953\n",
      "acc with max depth of 12 = 1.0\n",
      "acc with max depth of 16 = 1.0\n"
     ]
    }
   ],
   "source": [
    "def select_random_features(X, feature_count):\n",
    "    columns = list(X.columns)\n",
    "    np.random.shuffle(columns)\n",
    "    return X[columns[:feature_count]]\n",
    "\n",
    "max_depths = (4, 8, 12, 16)\n",
    "for md in max_depths:\n",
    "    rf_cls_model = RandomForestClassifier(n_estimators=7, criterion=\"gini\",max_depth=md)\n",
    "    rf_cls_model = rf_cls_model.fit(pd.get_dummies(X_train), y_train)\n",
    "    y_val_hat = rf_cls_model.predict(pd.get_dummies(X_val))\n",
    "\n",
    "    print(f'acc with max depth of {md} = {sklearn.metrics.accuracy_score(y_val, y_val_hat)}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
